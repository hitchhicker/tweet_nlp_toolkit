import pytest

from tweet_nlp_toolkit.prep.token import Token, WeiboToken
from tweet_nlp_toolkit.prep.tokenizer import TwitterTokenizer, white_space_tokenize, \
    SocialMediaTokenizer, WhiteSpaceTokenizer, social_media_tokenize, Detokenizer, chinese_tokenize, japanese_tokenize, \
    _is_chinese, _is_japanese, thai_tokenize, _is_thai, weibo_tokenize


def test_default_init_tokenizer():
    assert type(TwitterTokenizer()._tknzr) == SocialMediaTokenizer


def test_nltk_init_tokenizer():
    assert type(TwitterTokenizer(tokenizer='naive')._tknzr) == WhiteSpaceTokenizer
    assert type(TwitterTokenizer(tokenizer='social_media')._tknzr) == SocialMediaTokenizer


def test_error_init_tokenizer():
    with pytest.raises(NotImplementedError):
        _ = TwitterTokenizer(tokenizer='xxx')


@pytest.mark.parametrize(("text", "expected_tokens"),
                         [(" @remy: This is waaaaayyyy too much for you",
                           ['@remy:', 'This', 'is', 'waaaaayyyy', 'too', 'much', 'for', 'you']),
                          ("", [])])
def test_white_space_tokenize(text, expected_tokens):
    assert white_space_tokenize(text) == expected_tokens


@pytest.mark.parametrize(("text", "expected_tokens"),
                         [(
                                 "12 ab-cd -> 12.34&pound;100 ... test@gmail.com<p> @nlp https://www.google.fr CANT WAIT </p> ğŸ˜° ğŸ”¥ğŸ”¥ #TwinPeaks ï¼¼(^o^)ï¼! doesn\u2019t \ud83c\udf08",
                                 ['12', 'ab-cd', '->', '12.34', 'Â£', '100', '...', 'test@gmail.com',
                                  '<p>', '@nlp',
                                  'https://www.google.fr', 'CANT', 'WAIT', '</p>', 'ğŸ˜°', 'ğŸ”¥', 'ğŸ”¥',
                                  '#TwinPeaks',
                                  'ï¼¼(^o^)ï¼', '!', 'doesn', 'â€™', 't', 'ğŸŒˆ']
                         ),
                             ("", []),
                             (" à¸„à¸¥à¸±à¸šà¸à¸²à¸£à¸²à¹„à¸”à¸‹à¹Œ, à¸ˆà¸°à¸–à¸¹à¸à¸•à¹‰à¸­à¸‡. à¸§à¸±à¸™à¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢à¸—à¸¸à¸à¸ªà¸´à¹ˆà¸‡à¸—à¸µà¹ˆà¸”à¸¹à¹€à¸«à¸¡à¸·à¸­à¸™à¸§à¹ˆà¸²à¸•à¸à¸¥à¸‡",
                              ["à¸„à¸¥à¸±à¸šà¸à¸²à¸£à¸²à¹„à¸”à¸‹à¹Œ", ",", "à¸ˆà¸°à¸–à¸¹à¸à¸•à¹‰à¸­à¸‡", ".", "à¸§à¸±à¸™à¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢à¸—à¸¸à¸à¸ªà¸´à¹ˆà¸‡à¸—à¸µà¹ˆà¸”à¸¹à¹€à¸«à¸¡à¸·à¸­à¸™à¸§à¹ˆà¸²à¸•à¸à¸¥à¸‡"])])
def test_social_media_tokenize(text, expected_tokens):
    tokens = social_media_tokenize(text)
    assert tokens == expected_tokens


def test_detokenizer():
    en_detok = Detokenizer()
    assert en_detok.detokenize(['Hello', 'World', '!']) == 'Hello World!'
    assert en_detok.detokenize(
        'Paris , Lyon , and Grenoble .'.split(' ')) == 'Paris, Lyon, and Grenoble.'

    text = 'RÃ©formes des retraites : Macron "n\'est pas...\", dit Mazerolle'
    fr_detok = Detokenizer(lang='fr')
    assert fr_detok.detokenize(
        ['RÃ©formes', 'des', 'retraites', ':', 'Macron', '"', "n'est", 'pas', '...', '"', ',', 'dit',
         'Mazerolle']) == text

    # check that the tokenization is reversible by the detokenization
    assert fr_detok.detokenize(list(map(str, social_media_tokenize(text)))) == text


@pytest.mark.parametrize(("text", "expected_tokens"),
                         [
                             ("", []),  # empty input
                             (
                                     "æˆ‘è¿™ä¸ªæ˜ŸæœŸå…­å·¥ä½œ",  # plain Chinese
                                     ["æˆ‘", "è¿™ä¸ª", "æ˜ŸæœŸå…­", "å·¥ä½œ"]
                             ),
                             (
                                     "#996 oh my god ! æˆ‘è¿™ä¸ªæ˜ŸæœŸå…­å·¥ä½œ post english",  # mix English and Chinese
                                     ["#996", "oh", "my", "god", "!", "æˆ‘", "è¿™ä¸ª", "æ˜ŸæœŸå…­", "å·¥ä½œ", "post", "english"]
                             )])
def test_chinese_tokenize(text, expected_tokens):
    assert chinese_tokenize(text) == expected_tokens


@pytest.mark.parametrize(("text", "expected_tokens"),
                         [
                             ("", []),  # empty input
                             (
                                     "ç§ã¯åœŸæ›œæ—¥ã«å‡ºå‹¤ã—ã¾ã™ã€‚",  # plain Japanese
                                     ['ç§', 'ã¯', 'åœŸæ›œæ—¥', 'ã«', 'å‡ºå‹¤', 'ã—', 'ã¾ã™', 'ã€‚']
                             ),
                             (
                                     "#996 oh my god ! ç§ã¯åœŸæ›œæ—¥ã«å‡ºå‹¤ã—ã¾ã™ã€‚ post english",  # mix English and Japanese
                                     ["#996", "oh", "my", "god", "!", 'ç§', 'ã¯', 'åœŸæ›œæ—¥', 'ã«', 'å‡ºå‹¤', 'ã—', 'ã¾ã™', 'ã€‚',
                                      "post",
                                      "english"]
                             )])
def test_japanese_tokenize(text, expected_tokens):
    assert japanese_tokenize(text) == expected_tokens


@pytest.mark.parametrize(("text", "expected_tokens"),
                         [
                             ("", []),  # empty input
                             (
                                     "à¸‰à¸±à¸™à¸—à¸³à¸‡à¸²à¸™à¸§à¸±à¸™à¹€à¸ªà¸²à¸£à¹Œà¸™à¸µà¹‰",  # plain Thai
                                     ['à¸‰à¸±à¸™', 'à¸—à¸³à¸‡à¸²à¸™', 'à¸§à¸±à¸™', 'à¹€à¸ªà¸²à¸£à¹Œ', 'à¸™à¸µà¹‰']
                             ),
                             (
                                     "#996 oh my god ! à¸‰à¸±à¸™à¸—à¸³à¸‡à¸²à¸™à¸§à¸±à¸™à¹€à¸ªà¸²à¸£à¹Œà¸™à¸µà¹‰ post english",  # mix English and Thai
                                     ["#996", "oh", "my", "god", "!", 'à¸‰à¸±à¸™', 'à¸—à¸³à¸‡à¸²à¸™', 'à¸§à¸±à¸™', 'à¹€à¸ªà¸²à¸£à¹Œ', 'à¸™à¸µà¹‰', "post",
                                      "english"]
                             )])
def test_thai_tokenizer(text, expected_tokens):
    assert thai_tokenize(text) == expected_tokens


@pytest.mark.parametrize(("cp", "expected"),
                         [(ord("ã¯"), True),
                          (ord("å·¥"), True)])
def test_is_japanese(cp, expected):
    assert _is_japanese(cp) == expected


@pytest.mark.parametrize(("cp", "expected"),
                         [(ord("ã¯"), False),
                          (ord("å·¥"), True)])
def test_is_chinese(cp, expected):
    assert _is_chinese(cp) == expected


def test_is_thai():
    assert _is_thai(ord("à¸ª")) is True


@pytest.mark.parametrize(
    ("text", "expected"),
    [("#å…¨å›½å·²ç¡®è¯Šæ–°å‹è‚ºç‚ç—…ä¾‹319ä¾‹#ä¸­å›½åŠ æ²¹!", ["#å…¨å›½å·²ç¡®è¯Šæ–°å‹è‚ºç‚ç—…ä¾‹319ä¾‹#", "ä¸­å›½", "åŠ æ²¹", "!"])]
)
def test_weibo_tokenize_with_segment_hashtag_as_false(text, expected):
    assert weibo_tokenize(text, segment_hashtag=False) == expected


@pytest.mark.parametrize(
    ("text", "expected"),
    [("#å…¨å›½å·²ç¡®è¯Šæ–°å‹è‚ºç‚ç—…ä¾‹319ä¾‹#ä¸­å›½åŠ æ²¹!", ["#", "å…¨å›½", "å·²", "ç¡®è¯Š", "æ–°å‹", "è‚ºç‚", "ç—…ä¾‹", "319", "ä¾‹", "#", "ä¸­å›½", "åŠ æ²¹", "!"])]
)
def test_weibo_tokenize_with_segment_hashtag_as_true(text, expected):
    assert weibo_tokenize(text, segment_hashtag=True) == expected


def test_social_media_tokenize_return_type():
    assert type(social_media_tokenize("unit test")[0]) == Token


def test_white_space_tokenize_return_type():
    assert type(white_space_tokenize("unit test")[0]) == Token


def test_weibo_tokenize_return_type():
    assert type(weibo_tokenize("unit test")[0]) == WeiboToken
